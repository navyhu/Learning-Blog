Gradient Descent算法的直观理解

在linear regression学习算法中，cost function为
J(θ) = 1/2 * sum<i, m>(h(xi) - yi)^2

学习算法的目标是通过调整参数θ(以及bias)使J(θ)的对所有training data取值最小化

几乎所有书都会从几何角度直观解释Gradient Descent算法，就是在J(θ)的图形中随机选择一个起始点，
通过偏导数找出该点处最陡的路线，调整θ取值，完成一次gradient descent迭代，重复直到J(θ)变化值小于某值（ε）
其中对于θ的更新使用如下等式，α为learning rate
θj := θj - α * (∂J(θ) / ∂θj)

算法的几何很好理解，但是我一直没办法把这个更新θ的等式与几何解释联系起来，θ为什么要这么更新，后面的偏导数代表什么？
一晚的冥思苦想终于摸到点门道，J(θ)相对于θj的偏导数是J(θ)沿最陡路线下降的θj方向的变化，也就是说要想使J(θ)变小，
θj要调整的方向和大小。它的正负代表方向，值大小表明当前θ值的偏差度，以及θj方向对于学习算法效率的影响的大小。
